---
layout: post
title:  "Natural GD"
date:   2026-01-04
categories: [Learning]
---

Consider a Gaussian model $Y \mid f(\theta) \sim N( f(\theta), I)$. 
The negative log-likelihood is then

$$
\ell(\theta) = -\log p(Y, \theta) = \frac{1}{2} \| f(\theta) - Y \|^2 + \text{const}. 
$$

Notice that this is precisely the MSE. 

<br>

Let $\nabla_\theta \ell = J^\top \cdot (f(\theta) - Y) \triangleq J^\top \cdot e$. 
So $e = f(\theta)-Y \sim N(0, I)$. 

The Fisher information matrix is

$$
F = \mathbb{E}_Y [ (\nabla_\theta \ell) (\nabla_\theta \ell)^\top  ] = \mathbb{E}[ J^\top e e^\top J ]= J^\top \mathbb{E}[ e e^\top ] J = J^\top J. 
$$


Thus, for general MSE loss, under the Gaussian assumption, the NGD update is 

$$
\theta^+ = \theta - \eta (J^\top J)^{-1} \nabla_\theta \ell. 
$$

Note $J$ is shape $n_{y} \times n_{\text{param}}$. So with overparametrization, $J^\top J$ is singular. 
One can use pseudo inverse instead. 

<br>

Let's check the linear stability of NGD at a minimizer $\theta^\*$, i.e., $f(\theta^\*)=y$. 

The Jacobian of the NGD update map is

$$
\partial_\theta (\theta^+) = I - \eta \partial_\theta( (J^\top J)^{\dagger} J^\top e  ) = I - \eta \partial_\theta( (J^\top J)^{\dagger} J^\top  )\cdot  e - \eta (J^\top J)^{\dagger} J^\top \cdot \partial_\theta e. 
$$

Evaluated at a minimizer $\theta^\*$, we have $e^\*=0$ and thus 

$$
\partial_\theta (\theta^+)\mid_{\theta^\*} = I - \eta (J^\top J)^{\dagger} J^\top J. 
$$

Note that, importantly, $\Pi = (J^\top J)^{\dagger} J^\top J$ is the orthogonal projection onto the row space of $A$;
If the SVD of $J$ is $U\Sigma V^\top$, the SVD of $\Pi$ is $V \text{Diag}(I_r, 0)$ V^\top$. 

<br>

Consequently, the eigenvalues of $\partial_\theta (\theta^+)\mid_{\theta^\*}$ are $1$ and $1-\eta$, which does not depend on the location $\theta^\*$. 

<br>

This means that, from the eyes of NGD, all minimizers have the same sharpness. 


