---
layout: post
title:  "Gram vs Covariance"
date:   2025-07-23
categories: [Learning]
---

Consider a data matrix $X\in\mathbb{R}^{n\times d}$ with $n$ data points and $d$ features. Let $x_i$ denote a row vector / a data point.

<br>

The (data) Gram matrix $XX^T \in \mathbb{R}^{n\times n}$ captures the Euclidean geometry (lengths and angles) of the $n$ data points in the $d$ dimensional feature space. 

<br>

Now let's consider the matrix $X^TX \in \mathbb{R}^{d\times d}$. This is closely related to the sample second moment matrix, which is $\frac{1}{n}X^TX$, and the sample covariance matrix, 
which is $\frac{1}{n-1}X^TX$ if the data is centered ($\sum_i x_i=0\in\mathbb{R}^d$). 

We can think of features (columns) as functions, defined on the data space. In this sense, the matrix $X^TX$ is the Gram matrix of the discretized feature functions. 
